import os
import sys
from transformers import AutoTokenizer, AutoModel
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# ç¡®ä¿æ¨¡å‹å’Œæ•°æ®åº“è·¯å¾„æ­£ç¡®
# ä½¿ç”¨ os.path.join æ‹¼æ¥ç›¸å¯¹è·¯å¾„ï¼Œè¿™æ ·å¯ä»¥è·¨å¹³å°å·¥ä½œ
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, "chatglm3-6b")
DB_PATH = os.path.join(BASE_DIR, "titan_db")

# åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
def load_model_and_tokenizer():
    """
    åŠ è½½ ChatGLM3-6B æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    """
    print("ğŸš€ æ­£åœ¨åŠ è½½ ChatGLM3-6B æ¨¡å‹...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True).half().cuda()
        model = model.eval()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return tokenizer, model
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿ä½ å·²ç»å°† chatglm3-6b æ¨¡å‹æ–‡ä»¶å¤¹æ”¾ç½®åœ¨é¡¹ç›®æ ¹ç›®å½•ã€‚")
        sys.exit(1)

# åŠ è½½å‘é‡æ•°æ®åº“
def load_vector_db():
    """
    åŠ è½½ FAISS å‘é‡æ•°æ®åº“ã€‚
    """
    print("ğŸ“¦ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“...")
    embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-large-zh")
    try:
        db = FAISS.load_local(DB_PATH, embedding_model, allow_dangerous_deserialization=True)
        print("âœ… æ•°æ®åº“åŠ è½½æˆåŠŸï¼")
        return db
    except Exception as e:
        print(f"âŒ æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿ä½ å·²ç»è¿è¡Œè¿‡ shentan.py æ¥åˆ›å»ºæ•°æ®åº“ã€‚")
        sys.exit(1)

# è¿›è¡Œé—®ç­”
def chat_with_model(db, tokenizer, model):
    """
    å¯åŠ¨ä¸€ä¸ªé—®ç­”å¾ªç¯ã€‚
    """
    print("\n--- æœ¬åœ°é—®ç­”ç¨‹åºå·²å¯åŠ¨ ---")
    print("è¾“å…¥ä½ çš„é—®é¢˜ï¼Œæˆ–è¾“å…¥ 'exit' é€€å‡ºã€‚")

    while True:
        query = input("\nğŸ‘¤ ä½ : ")
        if query.lower() == 'exit':
            break

        if not query:
            continue

        print("ğŸ¤– æ­£åœ¨æ€è€ƒä¸­...")

        # åœ¨æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = db.similarity_search(query, k=3)
        context = "\n".join([doc.page_content for doc in docs])

        # æ„å»ºå‘é€ç»™æ¨¡å‹çš„prompt
        prompt = f"åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å›ç­”é—®é¢˜ã€‚\nçŸ¥è¯†åº“: {context}\né—®é¢˜: {query}\nå›ç­”:"

        # ç”Ÿæˆå›ç­”
        response, history = model.chat(tokenizer, prompt, history=[])
        print(f"ğŸ¤– TitanGPT: {response}")

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MODEL_PATH}")
        print("ğŸ’¡ è¯·ç¡®ä¿ä½ å·²ç»å°† chatglm3-6b æ¨¡å‹æ–‡ä»¶å¤¹æ”¾ç½®åœ¨é¡¹ç›®æ ¹ç›®å½•ã€‚")
        sys.exit(1)

    if not os.path.exists(DB_PATH):
        print(f"âŒ æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨: {DB_PATH}")
        print("ğŸ’¡ è¯·ç¡®ä¿ä½ å·²ç»è¿è¡Œè¿‡ shentan.py æ¥åˆ›å»º 'titan_db' æ–‡ä»¶å¤¹ã€‚")
        sys.exit(1)
        
    tokenizer, model = load_model_and_tokenizer()
    db = load_vector_db()

    chat_with_model(db, tokenizer, model)

    print("\nç¨‹åºå·²é€€å‡ºã€‚")
