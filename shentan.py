import os
import sys
import torch
from transformers import AutoTokenizer, AutoModel
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# åŠ¨æ€è·å–åŸºç¡€è·¯å¾„ï¼Œä¿è¯è·¨å¹³å°å…¼å®¹æ€§
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, "chatglm3-6b")
DB_PATH = os.path.join(BASE_DIR, "titan_db")

# æ£€æŸ¥è®¾å¤‡ç±»å‹å¹¶é€‰æ‹©åˆé€‚çš„è®¾å¤‡
DEVICE = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"

def load_model_and_tokenizer():
    """
    åŠ è½½ ChatGLM3-6B æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚
    """
    print(f"ğŸš€ æ­£åœ¨åŠ è½½ ChatGLM3-6B æ¨¡å‹åˆ° {DEVICE}...")
    try:
        # Load the model with appropriate precision based on device
        model = AutoModel.from_pretrained(MODEL_PATH, trust_remote_code=True)
        if DEVICE == "cuda":
            model = model.half().to(DEVICE)
        elif DEVICE == "mps":
            # MPS does not support half precision for all operations, using float32 instead
            model = model.to(DEVICE)
        else:
            model = model.to(DEVICE)

        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
        model = model.eval()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        return tokenizer, model
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"ğŸ’¡ è¯·æ£€æŸ¥ä»¥ä¸‹è·¯å¾„å’Œæ–‡ä»¶ï¼š\n- æ¨¡å‹è·¯å¾„: {MODEL_PATH}\n- ç¡®ä¿ä½ å·²ç»å®‰è£…äº†æ­£ç¡®çš„åº“ï¼ˆå¦‚ torch, transformers, accelerateï¼‰\n- å¦‚æœä½ åœ¨ä½¿ç”¨GPUï¼Œè¯·ç¡®ä¿é©±åŠ¨å’ŒCUDAç¯å¢ƒæ­£ç¡®ã€‚")
        sys.exit(1)

def load_vector_db():
    """
    åŠ è½½ FAISS å‘é‡æ•°æ®åº“ã€‚
    """
    print("ğŸ“¦ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“...")
    try:
        embedding_model = HuggingFaceEmbeddings(model_name="BAAI/bge-large-zh")
        db = FAISS.load_local(DB_PATH, embedding_model, allow_dangerous_deserialization=True)
        print("âœ… æ•°æ®åº“åŠ è½½æˆåŠŸï¼")
        return db
    except Exception as e:
        print(f"âŒ æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
        print(f"ğŸ’¡ è¯·ç¡®ä¿ä½ å·²ç»è¿è¡Œè¿‡ shentan.py æ¥åˆ›å»ºæ•°æ®åº“ã€‚æ•°æ®åº“è·¯å¾„: {DB_PATH}")
        sys.exit(1)

def chat_with_model(db, tokenizer, model):
    """
    å¯åŠ¨ä¸€ä¸ªé—®ç­”å¾ªç¯ã€‚
    """
    print("\n--- æœ¬åœ°é—®ç­”ç¨‹åºå·²å¯åŠ¨ ---")
    print("è¾“å…¥ä½ çš„é—®é¢˜ï¼Œæˆ–è¾“å…¥ 'exit' é€€å‡ºã€‚")

    while True:
        query = input("\nğŸ‘¤ ä½ : ")
        if query.lower() == 'exit':
            break

        if not query:
            continue

        print("ğŸ¤– æ­£åœ¨æ€è€ƒä¸­...")

        # åœ¨æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = db.similarity_search(query, k=3)
        context = "\n".join([doc.page_content for doc in docs])

        # æ„å»ºå‘é€ç»™æ¨¡å‹çš„prompt
        prompt = f"åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å›ç­”é—®é¢˜ã€‚\nçŸ¥è¯†åº“: {context}\né—®é¢˜: {query}\nå›ç­”:"

        # ç”Ÿæˆå›ç­”
        response, history = model.chat(tokenizer, prompt, history=[])
        print(f"ğŸ¤– TitanGPT: {response}")

# ä¸»ç¨‹åºå…¥å£
if __name__ == "__main__":
    if not os.path.exists(MODEL_PATH):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {MODEL_PATH}")
        print("ğŸ’¡ è¯·ç¡®ä¿ä½ å·²ç»å°† chatglm3-6b æ¨¡å‹æ–‡ä»¶å¤¹æ”¾ç½®åœ¨é¡¹ç›®æ ¹ç›®å½•ã€‚")
        sys.exit(1)

    if not os.path.exists(DB_PATH):
        print(f"âŒ æ•°æ®åº“è·¯å¾„ä¸å­˜åœ¨: {DB_PATH}")
        print("ğŸ’¡ è¯·ç¡®ä¿ä½ å·²ç»è¿è¡Œè¿‡ shentan.py æ¥åˆ›å»º 'titan_db' æ–‡ä»¶å¤¹ã€‚")
        sys.exit(1)
        
    tokenizer, model = load_model_and_tokenizer()
    db = load_vector_db()

    chat_with_model(db, tokenizer, model)

    print("\nç¨‹åºå·²é€€å‡ºã€‚")
